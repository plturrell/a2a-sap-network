"""
{{ description }}
Generated by Agent Builder Agent on {{ generated_at }}
"""

import asyncio
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from uuid import uuid4
import logging
import time

from app.a2a.sdk import (
    A2AAgentBase, a2a_handler, a2a_skill, a2a_task,
    A2AMessage, MessageRole, create_agent_id
)
from app.a2a.sdk.utils import create_success_response, create_error_response
from app.a2a.core.workflowContext import workflowContextManager
from app.a2a.core.workflowMonitor import workflowMonitor
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Data processing specific dependencies
import pandas as pd
import numpy as np
from pydantic import BaseModel, ValidationError

logger = logging.getLogger(__name__)


class {{ agent_name.replace(' ', '').replace('_', '') }}SDK(A2AAgentBase):
    """
    {{ agent_name }} - SDK Version
    {{ description }}
    """
    
    def __init__(self, base_url: str, **kwargs):
        super().__init__(
            agent_id="{{ agent_id }}",
            name="{{ agent_name }}",
            description="{{ description }}",
            version="1.0.0",
            base_url=base_url
        )
        
        # Configuration
        {% for key, value in configuration.items() %}
        self.{{ key }} = {{ value|tojson if value is string else value|tojson }}
        {% endfor %}
        
        # Data processing configuration
        self.batch_size = kwargs.get('batch_size', {{ default_batch_size|default(1000) }})
        self.max_file_size = kwargs.get('max_file_size', "{{ max_file_size|default('10MB') }}")
        
        # Prometheus metrics
        self.tasks_completed = Counter('a2a_agent_tasks_completed_total', 'Total completed tasks', ['agent_id', 'task_type'])
        self.tasks_failed = Counter('a2a_agent_tasks_failed_total', 'Total failed tasks', ['agent_id', 'task_type'])
        self.processing_time = Histogram('a2a_agent_processing_time_seconds', 'Task processing time', ['agent_id', 'task_type'])
        self.queue_depth = Gauge('a2a_agent_queue_depth', 'Current queue depth', ['agent_id'])
        self.skills_count = Gauge('a2a_agent_skills_count', 'Number of skills available', ['agent_id'])
        self.rows_processed = Counter('a2a_agent_rows_processed_total', 'Total rows processed', ['agent_id'])
        
        # Set initial metrics
        self.queue_depth.labels(agent_id=self.agent_id).set(0)
        self.skills_count.labels(agent_id=self.agent_id).set({{ skills|length }})
        
        # Start metrics server
        self._start_metrics_server()
        
        self.processing_stats = {
            "total_processed": 0,
            "rows_processed": 0,
            "files_processed": 0,
            {% for skill in skills %}
            "{{ skill }}_count": 0,
            {% endfor %}
        }
        
        logger.info(f"Initialized {self.name} with SDK v1.0.0")
    
    def _start_metrics_server(self):
        """Start Prometheus metrics server"""
        try:
            port = int(os.environ.get('PROMETHEUS_PORT', '8000'))
            start_http_server(port)
            logger.info(f"Started Prometheus metrics server on port {port}")
        except Exception as e:
            logger.warning(f"Failed to start metrics server: {e}")
    
    async def initialize(self) -> None:
        """Initialize agent resources"""
        logger.info("Initializing {{ agent_name }}...")
        
        # Initialize storage
        storage_path = os.getenv("AGENT_STORAGE_PATH", "/tmp/{{ agent_id }}_state")
        os.makedirs(storage_path, exist_ok=True)
        self.storage_path = storage_path
        
        # Initialize data processing workspace
        workspace_path = os.path.join(storage_path, "workspace")
        os.makedirs(workspace_path, exist_ok=True)
        self.workspace_path = workspace_path
        
        # Custom initialization logic
        await self._custom_initialization()
        
        logger.info("{{ agent_name }} initialization complete")
    
    async def _custom_initialization(self):
        """Custom initialization logic for data processing"""
        # Initialize pandas settings
        pd.set_option('display.max_columns', 50)
        pd.set_option('display.max_rows', 100)
        
        # Configure data processing parameters
        self.supported_formats = ['csv', 'json', 'xlsx', 'parquet']
        self.max_memory_usage = '1GB'

{% for handler in handlers %}
    @a2a_handler("{{ handler }}")
    async def handle_{{ handler }}(self, message: A2AMessage) -> Dict[str, Any]:
        """Handler for {{ handler }} requests"""
        start_time = time.time()
        
        try:
            # Extract request data
            request_data = self._extract_request_data(message)
            
            # Process request - implement your logic here
            result = await self._process_{{ handler }}(request_data, message.conversation_id)
            
            # Record success metrics
            self.tasks_completed.labels(agent_id=self.agent_id, task_type='{{ handler }}').inc()
            self.processing_time.labels(agent_id=self.agent_id, task_type='{{ handler }}').observe(time.time() - start_time)
            
            return create_success_response(result)
            
        except Exception as e:
            # Record failure metrics
            self.tasks_failed.labels(agent_id=self.agent_id, task_type='{{ handler }}').inc()
            logger.error(f"{{ handler }} failed: {e}")
            return create_error_response(f"{{ handler }} failed: {str(e)}")
    
    async def _process_{{ handler }}(self, request_data: Dict[str, Any], context_id: str) -> Dict[str, Any]:
        """Process {{ handler }} request - implement your logic here"""
        {% if handler == 'data_processing' %}
        # Data processing specific implementation
        data_source = request_data.get('data_source')
        processing_config = request_data.get('config', {})
        
        if not data_source:
            raise ValueError("data_source is required for data processing")
        
        # Load and process data
        df = await self._load_data(data_source)
        processed_df = await self._apply_transformations(df, processing_config)
        
        # Save results
        output_path = await self._save_processed_data(processed_df, context_id)
        
        return {
            "message": "{{ handler }} processed successfully", 
            "context_id": context_id,
            "rows_processed": len(processed_df),
            "output_path": output_path,
            "processing_time": time.time() - start_time
        }
        {% elif handler == 'batch_processing' %}
        # Batch processing specific implementation
        batch_config = request_data.get('batch_config', {})
        data_batches = request_data.get('batches', [])
        
        results = []
        for i, batch in enumerate(data_batches):
            batch_result = await self._process_batch(batch, batch_config)
            results.append(batch_result)
            
            # Update progress
            if i % 10 == 0:
                logger.info(f"Processed {i+1}/{len(data_batches)} batches")
        
        return {
            "message": "{{ handler }} completed successfully",
            "context_id": context_id,
            "batches_processed": len(results),
            "results": results
        }
        {% else %}
        # Generic handler implementation
        return {"message": "{{ handler }} processed successfully", "context_id": context_id}
        {% endif %}

{% endfor %}

{% for skill in skills %}
    @a2a_skill("{{ skill }}")
    async def {{ skill }}_skill(self, *args, **kwargs) -> Dict[str, Any]:
        """{{ skill }} skill implementation"""
        
        {% if skill == 'data_validation' %}
        # Data validation implementation
        data = kwargs.get('data')
        validation_rules = kwargs.get('rules', {})
        
        if data is None:
            raise ValueError("Data is required for validation")
        
        validation_results = {
            'valid_rows': 0,
            'invalid_rows': 0,
            'errors': [],
            'warnings': []
        }
        
        # Perform validation
        if isinstance(data, pd.DataFrame):
            # Validate DataFrame
            for rule_name, rule_config in validation_rules.items():
                validation_result = self._apply_validation_rule(data, rule_name, rule_config)
                validation_results['errors'].extend(validation_result.get('errors', []))
                validation_results['warnings'].extend(validation_result.get('warnings', []))
            
            validation_results['valid_rows'] = len(data) - len(validation_results['errors'])
            validation_results['invalid_rows'] = len(validation_results['errors'])
        
        self.processing_stats["{{ skill }}_count"] += 1
        return validation_results
        
        {% elif skill == 'data_transformation' %}
        # Data transformation implementation
        data = kwargs.get('data')
        transformations = kwargs.get('transformations', [])
        
        if data is None:
            raise ValueError("Data is required for transformation")
        
        transformed_data = data.copy() if hasattr(data, 'copy') else data
        
        # Apply transformations
        for transformation in transformations:
            transformation_type = transformation.get('type')
            transformation_config = transformation.get('config', {})
            
            if transformation_type == 'filter':
                transformed_data = self._apply_filter(transformed_data, transformation_config)
            elif transformation_type == 'aggregate':
                transformed_data = self._apply_aggregation(transformed_data, transformation_config)
            elif transformation_type == 'join':
                transformed_data = self._apply_join(transformed_data, transformation_config)
            elif transformation_type == 'normalize':
                transformed_data = self._apply_normalization(transformed_data, transformation_config)
        
        self.processing_stats["{{ skill }}_count"] += 1
        return {
            "result": transformed_data,
            "transformations_applied": len(transformations),
            "timestamp": datetime.now().isoformat()
        }
        
        {% elif skill == 'data_quality_check' %}
        # Data quality check implementation
        data = kwargs.get('data')
        quality_metrics = kwargs.get('metrics', ['completeness', 'accuracy', 'consistency'])
        
        if data is None:
            raise ValueError("Data is required for quality check")
        
        quality_results = {}
        
        if isinstance(data, pd.DataFrame):
            # Calculate quality metrics
            if 'completeness' in quality_metrics:
                quality_results['completeness'] = self._calculate_completeness(data)
            if 'accuracy' in quality_metrics:
                quality_results['accuracy'] = self._calculate_accuracy(data)
            if 'consistency' in quality_metrics:
                quality_results['consistency'] = self._calculate_consistency(data)
            
            # Overall quality score
            quality_results['overall_score'] = sum(quality_results.values()) / len(quality_results)
        
        self.processing_stats["{{ skill }}_count"] += 1
        return quality_results
        
        {% else %}
        # Generic skill implementation
        self.processing_stats["{{ skill }}_count"] += 1
        return {"result": "{{ skill }} completed", "timestamp": datetime.now().isoformat()}
        {% endif %}

{% endfor %}

{% for task in tasks %}
    @a2a_task(
        task_type="{{ task }}",
        description="{{ task }} task implementation",
        timeout=300,
        retry_attempts=2
    )
    async def {{ task }}_task(self, *args, **kwargs) -> Dict[str, Any]:
        """{{ task }} task implementation"""
        
        try:
            {% if task == 'process_dataset' %}
            # Process dataset task implementation
            dataset_path = kwargs.get('dataset_path')
            processing_config = kwargs.get('config', {})
            
            if not dataset_path:
                raise ValueError("dataset_path is required")
            
            # Load dataset
            df = await self._load_data(dataset_path)
            
            # Apply processing
            processed_df = await self._apply_transformations(df, processing_config)
            
            # Update metrics
            self.rows_processed.labels(agent_id=self.agent_id).inc(len(processed_df))
            self.processing_stats["files_processed"] += 1
            
            result = {
                "rows_processed": len(processed_df),
                "columns": list(processed_df.columns),
                "processing_config": processing_config
            }
            
            {% elif task == 'validate_data' %}
            # Validate data task implementation
            data = kwargs.get('data')
            validation_config = kwargs.get('validation_config', {})
            
            if data is None:
                raise ValueError("data is required")
            
            # Perform validation using skill
            validation_result = await self.execute_skill("data_validation", data=data, rules=validation_config)
            
            result = {
                "validation_passed": len(validation_result.get('errors', [])) == 0,
                "validation_details": validation_result
            }
            
            {% elif task == 'transform_data' %}
            # Transform data task implementation
            data = kwargs.get('data')
            transformations = kwargs.get('transformations', [])
            
            if data is None:
                raise ValueError("data is required")
            
            # Perform transformation using skill
            transformation_result = await self.execute_skill("data_transformation", 
                                                           data=data, 
                                                           transformations=transformations)
            
            result = {
                "transformed_data": transformation_result.get('result'),
                "transformations_applied": transformation_result.get('transformations_applied')
            }
            
            {% else %}
            # Generic task implementation
            result = await self._execute_{{ task }}(*args, **kwargs)
            {% endif %}
            
            self.processing_stats["total_processed"] += 1
            
            return {
                "task_successful": True,
                "task": "{{ task }}",
                "result": result
            }
            
        except Exception as e:
            logger.error(f"{{ task }} task failed: {e}")
            return {
                "task_successful": False,
                "task": "{{ task }}",
                "error": str(e)
            }
    
    async def _execute_{{ task }}(self, *args, **kwargs) -> Dict[str, Any]:
        """Execute {{ task }} - implement your logic here"""
        return {"message": "{{ task }} executed successfully"}

{% endfor %}

    # Data processing helper methods
    async def _load_data(self, data_source: str) -> pd.DataFrame:
        """Load data from various sources"""
        if data_source.endswith('.csv'):
            return pd.read_csv(data_source)
        elif data_source.endswith('.json'):
            return pd.read_json(data_source)
        elif data_source.endswith('.xlsx'):
            return pd.read_excel(data_source)
        elif data_source.endswith('.parquet'):
            return pd.read_parquet(data_source)
        else:
            raise ValueError(f"Unsupported data format: {data_source}")
    
    async def _apply_transformations(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """Apply data transformations"""
        result_df = df.copy()
        
        # Example transformations
        if 'drop_nulls' in config and config['drop_nulls']:
            result_df = result_df.dropna()
        
        if 'columns_to_drop' in config:
            result_df = result_df.drop(columns=config['columns_to_drop'], errors='ignore')
        
        if 'rename_columns' in config:
            result_df = result_df.rename(columns=config['rename_columns'])
        
        return result_df
    
    async def _save_processed_data(self, df: pd.DataFrame, context_id: str) -> str:
        """Save processed data"""
        output_path = os.path.join(self.workspace_path, f"processed_data_{context_id}.csv")
        df.to_csv(output_path, index=False)
        return output_path
    
    def _calculate_completeness(self, df: pd.DataFrame) -> float:
        """Calculate data completeness score"""
        total_cells = df.shape[0] * df.shape[1]
        non_null_cells = df.count().sum()
        return non_null_cells / total_cells if total_cells > 0 else 0.0
    
    def _calculate_accuracy(self, df: pd.DataFrame) -> float:
        """Calculate data accuracy score (simplified)"""
        # This is a simplified implementation - you would implement domain-specific accuracy checks
        return 0.95  # Placeholder
    
    def _calculate_consistency(self, df: pd.DataFrame) -> float:
        """Calculate data consistency score"""
        # Check for consistent data types and formats
        consistency_scores = []
        for column in df.columns:
            if df[column].dtype == 'object':
                # Check string consistency (example: consistent date formats)
                unique_patterns = df[column].astype(str).apply(lambda x: len(x) if pd.notna(x) else 0).nunique()
                score = 1.0 / unique_patterns if unique_patterns > 0 else 1.0
                consistency_scores.append(min(score, 1.0))
            else:
                consistency_scores.append(1.0)  # Numeric columns are generally consistent
        
        return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0.0
    
    def _extract_request_data(self, message: A2AMessage) -> Dict[str, Any]:
        """Extract request data from message"""
        request_data = {}
        
        for part in message.parts:
            if part.kind == "data" and part.data:
                request_data.update(part.data)
            elif part.kind == "file" and part.file:
                request_data["file"] = part.file
        
        return request_data
    
    async def cleanup(self) -> None:
        """Cleanup agent resources"""
        try:
            # Save processing statistics
            stats_file = os.path.join(self.storage_path, "processing_stats.json")
            with open(stats_file, 'w') as f:
                json.dump(self.processing_stats, f, indent=2)
            
            logger.info(f"{{ agent_name }} cleanup completed")
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")