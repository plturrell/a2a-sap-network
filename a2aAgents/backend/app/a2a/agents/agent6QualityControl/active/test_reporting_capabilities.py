import asyncio
# Performance: Consider using asyncio.gather for concurrent operations
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List

from qualityControlManagerAgent import QualityControlManagerAgent


from app.a2a.core.security_base import SecureA2AAgent
#!/usr/bin/env python3
"""
Test script for Quality Control Manager Agent Reporting and Auditing Capabilities
Demonstrates comprehensive reporting, compliance assessment, and responsible AI auditing
"""

# A2A Protocol Compliance: Require environment variables
required_env_vars = ["A2A_SERVICE_URL", "A2A_SERVICE_HOST", "A2A_BASE_URL"]
missing_vars = [var for var in required_env_vars if var in locals() and not os.getenv(var)]
if missing_vars:
    raise ValueError(f"Required environment variables not set for A2A compliance: {missing_vars}")
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_sample_agent_database_data() -> Dict[str, Any]:
    """Create sample database data that would be stored by Agent 4 and 5"""

    # Sample test execution results from Agent 4 (Calculation Validation)
    agent4_test_results = [
        {
            "test_execution_batch": {
                "batch_id": "batch_001",
                "executed_at": "2024-01-15T10:30:00Z",
                "agent_id": "calc_validation_agent_4",
                "total_tests": 50,
                "passed_tests": 47,
                "failed_tests": 3,
                "results": [
                    {
                        "test_id": "calc_test_001",
                        "service_id": "calculation_service_A",
                        "success": True,
                        "actual_output": 42.0,
                        "execution_time": 1.2,
                        "validation_results": {
                            "passed": True,
                            "method": "exact",
                            "historical_pattern_score": 0.95
                        },
                        "quality_scores": {
                            "accuracy": 1.0,
                            "performance": 0.87,
                            "reliability": 1.0,
                            "overall": 0.92
                        }
                    },
                    {
                        "test_id": "calc_test_002",
                        "service_id": "calculation_service_B",
                        "success": False,
                        "actual_output": None,
                        "execution_time": 8.5,
                        "error_message": "timeout exceeded",
                        "validation_results": {
                            "passed": False,
                            "method": "timeout",
                        },
                        "quality_scores": {
                            "accuracy": 0.0,
                            "performance": 0.2,
                            "reliability": 0.0,
                            "overall": 0.07
                        }
                    }
                ]
            }
        },
        {
            "test_execution_batch": {
                "batch_id": "batch_002",
                "executed_at": "2024-01-15T14:15:00Z",
                "agent_id": "calc_validation_agent_4",
                "total_tests": 75,
                "passed_tests": 72,
                "failed_tests": 3,
                "results": [
                    {
                        "test_id": "calc_test_003",
                        "service_id": "calculation_service_A",
                        "success": True,
                        "actual_output": 123.45,
                        "execution_time": 0.8,
                        "quality_scores": {
                            "accuracy": 0.98,
                            "performance": 0.95,
                            "reliability": 1.0,
                            "overall": 0.94
                        }
                    }
                ]
            }
        }
    ]

    # Sample test execution results from Agent 5 (QA Validation)
    agent5_test_results = [
        {
            "test_execution_batch": {
                "batch_id": "qa_batch_001",
                "executed_at": "2024-01-15T11:00:00Z",
                "agent_id": "qa_validation_agent_5",
                "total_tests": 30,
                "passed_tests": 28,
                "failed_tests": 2,
                "results": [
                    {
                        "test_id": "qa_test_001",
                        "service_id": "qa_service_A",
                        "success": True,
                        "actual_output": "Correct factual answer",
                        "execution_time": 2.1,
                        "validation_results": {
                            "passed": True,
                            "method": "semantic_match",
                            "semantic_similarity_score": 0.91
                        },
                        "quality_scores": {
                            "accuracy": 0.91,
                            "performance": 0.83,
                            "reliability": 0.95,
                            "overall": 0.90
                        }
                    },
                    {
                        "test_id": "qa_test_002",
                        "service_id": "qa_service_B",
                        "success": False,
                        "actual_output": "Incorrect answer",
                        "execution_time": 3.2,
                        "validation_results": {
                            "passed": False,
                            "method": "validation_failure",
                        },
                        "quality_scores": {
                            "accuracy": 0.2,
                            "performance": 0.7,
                            "reliability": 0.3,
                            "overall": 0.4
                        }
                    }
                ]
            }
        }
    ]

    return {
        "calc_validation_agent_4": {
            "test_results": agent4_test_results,
            "quality_assessments": [],
            "performance_metrics": []
        },
        "qa_validation_agent_5": {
            "test_results": agent5_test_results,
            "quality_assessments": [],
            "performance_metrics": []
        }
    }


async def test_database_integration(agent: QualityControlManagerAgent):
    """Test database integration and data retrieval"""

    print(f"\n{'='*80}")
    print(f"Testing Database Integration and Data Retrieval")
    print(f"{'='*80}")

    # Mock the database retrieval by replacing the method
    sample_data = create_sample_agent_database_data()

    async def mock_retrieve_agent_data(agent_id: str, data_type: str, time_range=None):
        """Mock database retrieval for testing"""
        if agent_id in sample_data and data_type in sample_data[agent_id]:
            return sample_data[agent_id][data_type]
        return []

    # Replace the method temporarily for testing
    original_method = agent._retrieve_agent_data_from_database
    agent._retrieve_agent_data_from_database = mock_retrieve_agent_data

    try:
        print(f"\nğŸ“Š Testing Data Retrieval:")

        # Test Agent 4 data retrieval
        agent4_data = await agent._retrieve_agent_data_from_database(
            "calc_validation_agent_4", "test_results"
        )
        print(f"  Agent 4 test results: {len(agent4_data)} batches retrieved")

        # Test Agent 5 data retrieval
        agent5_data = await agent._retrieve_agent_data_from_database(
            "qa_validation_agent_5", "test_results"
        )
        print(f"  Agent 5 test results: {len(agent5_data)} batches retrieved")

        # Analyze data structure alignment
        print(f"\nğŸ” Data Structure Analysis:")
        if agent4_data:
            batch = agent4_data[0]
            batch_data = batch.get("test_execution_batch", {})
            print(f"  Agent 4 batch structure:")
            print(f"    - Batch ID: {batch_data.get('batch_id')}")
            print(f"    - Total tests: {batch_data.get('total_tests')}")
            print(f"    - Quality scores available: {'quality_scores' in batch_data.get('results', [{}])[0]}")

        if agent5_data:
            batch = agent5_data[0]
            batch_data = batch.get("test_execution_batch", {})
            print(f"  Agent 5 batch structure:")
            print(f"    - Batch ID: {batch_data.get('batch_id')}")
            print(f"    - Total tests: {batch_data.get('total_tests')}")
            print(f"    - Semantic scores available: {'semantic_similarity_score' in str(batch_data)}")

        return True

    finally:
        # Restore original method
        agent._retrieve_agent_data_from_database = original_method


async def test_comprehensive_reporting(agent: QualityControlManagerAgent):
    """Test comprehensive reporting capabilities"""

    print(f"\n{'='*80}")
    print(f"Testing Comprehensive Reporting Capabilities")
    print(f"{'='*80}")

    # Mock data retrieval for testing
    sample_data = create_sample_agent_database_data()

    async def mock_retrieve_agent_data(agent_id: str, data_type: str, time_range=None):
        if agent_id in sample_data and data_type in sample_data[agent_id]:
            return sample_data[agent_id][data_type]
        return []

    original_method = agent._retrieve_agent_data_from_database
    agent._retrieve_agent_data_from_database = mock_retrieve_agent_data

    try:
        # Test different report types
        report_types = ["summary", "detailed", "audit", "compliance"]

        for report_type in report_types:
            print(f"\nğŸ“‹ Testing {report_type.title()} Report:")

            try:
                result = await agent.generate_comprehensive_report(
                    report_type=report_type,
                    time_range={
                        "start_date": "2024-01-01",
                        "end_date": "2024-01-31"
                    },
                    include_agents=["calc_validation_agent_4", "qa_validation_agent_5"],
                    metrics_focus=["accuracy", "performance", "reliability"]
                )

                if result.get("success"):
                    report = result["report"]
                    print(f"  âœ… {report_type} report generated successfully")
                    print(f"    Report ID: {report['report_id']}")
                    print(f"    Agents analyzed: {report['metadata']['data_sources']}")
                    print(f"    Total records: {report['metadata']['total_records_analyzed']}")
                    print(f"    Report size: {result['summary']['report_size_kb']:.1f} KB")

                    # Show executive summary
                    exec_summary = report.get("executive_summary", {})
                    print(f"    System health: {exec_summary.get('system_health', 'Unknown')}")
                    print(f"    Success rate: {exec_summary.get('overall_success_rate', 0):.1%}")

                    # Show key insights for summary reports
                    if report_type == "summary":
                        content = report.get("content", {})
                        insights = content.get("key_insights", [])[:3]
                        print(f"    Key insights: {len(insights)} identified")
                        for insight in insights:
                            print(f"      â€¢ {insight}")

                    # Show audit findings for audit reports
                    elif report_type == "audit":
                        content = report.get("content", {})
                        findings = content.get("audit_findings", [])
                        print(f"    Audit findings: {len(findings)} issues")
                        data_integrity = content.get("data_integrity", {})
                        print(f"    Data integrity: {data_integrity.get('compliance_status', 'Unknown')}")

                    # Show compliance score for compliance reports
                    elif report_type == "compliance":
                        content = report.get("content", {})
                        compliance_score = content.get("compliance_score", 0)
                        print(f"    Compliance score: {compliance_score:.1%}")

                        responsible_ai = content.get("responsible_ai_assessment", {})
                        if responsible_ai:
                            print(f"    Responsible AI: {responsible_ai.get('compliance_level', 'Unknown')}")

                else:
                    print(f"  âŒ {report_type} report failed: {result.get('error')}")

            except Exception as e:
                print(f"  âŒ {report_type} report error: {e}")

    finally:
        agent._retrieve_agent_data_from_database = original_method


async def test_mcp_reporting_tools(agent: QualityControlManagerAgent):
    """Test MCP reporting tools"""

    print(f"\n{'='*80}")
    print(f"Testing MCP Reporting Tools")
    print(f"{'='*80}")

    # Mock data retrieval
    sample_data = create_sample_agent_database_data()

    async def mock_retrieve_agent_data(agent_id: str, data_type: str, time_range=None):
        if agent_id in sample_data and data_type in sample_data[agent_id]:
            return sample_data[agent_id][data_type]
        return []

    original_method = agent._retrieve_agent_data_from_database
    agent._retrieve_agent_data_from_database = mock_retrieve_agent_data

    try:
        # Test generate_quality_report MCP tool
        print(f"\nğŸ”§ Testing generate_quality_report MCP Tool:")

        mcp_result = await agent.generate_quality_report_mcp(
            report_type="summary",
            time_range={
                "start_date": "2024-01-01",
                "end_date": "2024-01-31"
            },
            include_agents=["calc_validation_agent_4", "qa_validation_agent_5"],
            metrics_focus=["accuracy", "performance"]
        )

        if mcp_result.get("success"):
            print(f"  âœ… Quality report MCP tool successful")
            print(f"    Report ID: {mcp_result.get('report_id')}")
            print(f"    Agents analyzed: {mcp_result['summary']['agents_analyzed']}")
            print(f"    Data points: {mcp_result['summary']['total_data_points']}")
        else:
            print(f"  âŒ Quality report MCP tool failed: {mcp_result.get('error')}")

        # Test generate_audit_summary MCP tool
        print(f"\nğŸ”§ Testing generate_audit_summary MCP Tool:")

        audit_summary = await agent.generate_audit_summary_mcp(
            time_range={
                "start_date": "2024-01-01",
                "end_date": "2024-01-31"
            },
            focus_areas=["compliance", "quality", "performance"]
        )

        if audit_summary.get("success"):
            print(f"  âœ… Audit summary MCP tool successful")
            summary = audit_summary["audit_summary"]
            print(f"    Audit ID: {summary['audit_summary']['audit_id']}")
            print(f"    Overall status: {summary['audit_summary']['overall_status']}")
            print(f"    Focus areas: {summary['audit_summary']['focus_areas']}")

            key_metrics = summary.get("key_metrics", {})
            print(f"    Data integrity: {key_metrics.get('data_integrity_score', 0):.1%}")
            print(f"    Process compliance: {key_metrics.get('process_compliance_score', 0):.1%}")
            print(f"    Risk level: {key_metrics.get('risk_level', 'Unknown')}")

            print(f"    Stakeholder message: {summary.get('stakeholder_message', 'N/A')[:100]}...")
        else:
            print(f"  âŒ Audit summary MCP tool failed: {audit_summary.get('error')}")

    finally:
        agent._retrieve_agent_data_from_database = original_method


async def test_responsible_ai_assessment(agent: QualityControlManagerAgent):
    """Test responsible AI assessment capabilities"""

    print(f"\n{'='*80}")
    print(f"Testing Responsible AI Assessment")
    print(f"{'='*80}")

    # Create sample agent data
    sample_data = create_sample_agent_database_data()

    # Test responsible AI assessment
    print(f"\nğŸ¤– Testing Responsible AI Assessment:")

    try:
        responsible_ai = await agent._assess_responsible_ai(sample_data)

        print(f"  Overall score: {responsible_ai.get('overall_score', 0):.1%}")
        print(f"  Compliance level: {responsible_ai.get('compliance_level', 'Unknown')}")

        # Test individual assessments
        assessments = ["fairness", "transparency", "accountability", "reliability"]
        for assessment in assessments:
            result = responsible_ai.get(assessment, {})
            score = result.get("score", 0)
            description = result.get("description", "No description")
            print(f"    {assessment.title()}: {score:.1%} - {description[:60]}...")

        print(f"  âœ… Responsible AI assessment completed")

    except Exception as e:
        print(f"  âŒ Responsible AI assessment failed: {e}")

    # Test bias detection
    print(f"\nğŸ” Testing Bias Detection:")

    try:
        bias_detection = await agent._detect_bias_patterns(sample_data)

        bias_detected = bias_detection.get("bias_detected", False)
        print(f"  Bias detected: {bias_detected}")

        analysis = bias_detection.get("bias_analysis", {})
        for key, value in analysis.items():
            print(f"    {key.replace('_', ' ').title()}: {value}")

        recommendations = bias_detection.get("recommendations", [])
        print(f"  Recommendations: {len(recommendations)}")
        for rec in recommendations[:2]:
            print(f"    â€¢ {rec}")

        print(f"  âœ… Bias detection completed")

    except Exception as e:
        print(f"  âŒ Bias detection failed: {e}")

    # Test transparency evaluation
    print(f"\nğŸ” Testing Transparency Evaluation:")

    try:
        transparency = await agent._evaluate_transparency(sample_data)

        score = transparency.get("transparency_score", 0)
        print(f"  Transparency score: {score:.1%}")

        measures = transparency.get("transparency_measures", {})
        for measure, status in measures.items():
            print(f"    {measure.replace('_', ' ').title()}: {status}")

        print(f"  âœ… Transparency evaluation completed")

    except Exception as e:
        print(f"  âŒ Transparency evaluation failed: {e}")


async def test_audit_capabilities(agent: QualityControlManagerAgent):
    """Test comprehensive audit capabilities"""

    print(f"\n{'='*80}")
    print(f"Testing Audit Capabilities")
    print(f"{'='*80}")

    sample_data = create_sample_agent_database_data()

    # Test data integrity audit
    print(f"\nğŸ”’ Testing Data Integrity Audit:")

    try:
        integrity_audit = await agent._audit_data_integrity(sample_data)

        score = integrity_audit.get("integrity_score", 0)
        status = integrity_audit.get("compliance_status", "Unknown")

        print(f"  Integrity score: {score:.1%}")
        print(f"  Compliance status: {status}")
        print(f"  Records checked: {integrity_audit.get('total_records_checked', 0)}")
        print(f"  Complete records: {integrity_audit.get('complete_records', 0)}")

        issues = integrity_audit.get("integrity_issues", [])
        if issues:
            print(f"  Issues found: {len(issues)}")
            for issue in issues[:3]:
                print(f"    â€¢ {issue}")
        else:
            print(f"  âœ… No integrity issues found")

    except Exception as e:
        print(f"  âŒ Data integrity audit failed: {e}")

    # Test process compliance audit
    print(f"\nğŸ“‹ Testing Process Compliance Audit:")

    try:
        compliance_audit = await agent._audit_process_compliance(sample_data)

        score = compliance_audit.get("overall_compliance_score", 0)
        status = compliance_audit.get("compliance_status", "Unknown")

        print(f"  Overall compliance score: {score:.1%}")
        print(f"  Compliance status: {status}")

        checks = compliance_audit.get("compliance_checks", {})
        for check_name, check_result in checks.items():
            check_status = check_result.get("status", "Unknown")
            description = check_result.get("description", "No description")
            print(f"    {check_name.replace('_', ' ').title()}: {check_status} - {description}")

    except Exception as e:
        print(f"  âŒ Process compliance audit failed: {e}")

    # Test risk assessment
    print(f"\nâš ï¸  Testing Risk Assessment:")

    try:
        risk_assessment = await agent._conduct_risk_assessment(sample_data)

        risk_level = risk_assessment.get("overall_risk_level", "Unknown")
        print(f"  Overall risk level: {risk_level}")

        risks = risk_assessment.get("identified_risks", [])
        print(f"  Identified risks: {len(risks)}")

        for risk in risks:
            print(f"    â€¢ {risk.get('risk_type')} ({risk.get('severity')}) - {risk.get('description')}")

        recommendations = risk_assessment.get("risk_mitigation_recommendations", [])
        print(f"  Mitigation recommendations: {len(recommendations)}")
        for rec in recommendations[:2]:
            print(f"    â€¢ {rec}")

    except Exception as e:
        print(f"  âŒ Risk assessment failed: {e}")


async def test_cross_agent_analysis(agent: QualityControlManagerAgent):
    """Test cross-agent analysis capabilities"""

    print(f"\n{'='*80}")
    print(f"Testing Cross-Agent Analysis")
    print(f"{'='*80}")

    sample_data = create_sample_agent_database_data()

    try:
        cross_analysis = await agent._perform_cross_agent_analysis(sample_data)

        print(f"\nğŸ“Š Cross-Agent Performance Comparison:")

        success_rates = cross_analysis.get("agent_success_rates", {})
        for agent_id, rate in success_rates.items():
            print(f"  {agent_id}: {rate:.1%} success rate")

        best_agent = cross_analysis.get("best_performing_agent")
        worst_agent = cross_analysis.get("worst_performing_agent")
        variance = cross_analysis.get("performance_variance", 0)

        print(f"\nğŸ† Performance Analysis:")
        print(f"  Best performing agent: {best_agent}")
        print(f"  Worst performing agent: {worst_agent}")
        print(f"  Performance variance: {variance:.1%}")

        recommendations = cross_analysis.get("recommendations", [])
        print(f"\nğŸ’¡ Cross-Agent Recommendations:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")

        print(f"  âœ… Cross-agent analysis completed")

    except Exception as e:
        print(f"  âŒ Cross-agent analysis failed: {e}")


async def test_metrics_alignment(agent: QualityControlManagerAgent):
    """Test metrics alignment with agent skills"""

    print(f"\n{'='*80}")
    print(f"Testing Metrics Alignment with Agent Skills")
    print(f"{'='*80}")

    sample_data = create_sample_agent_database_data()

    print(f"\nğŸ“ Analyzing Metrics Alignment:")

    # Extract metrics from Agent 4 data
    agent4_data = sample_data.get("calc_validation_agent_4", {})
    agent4_tests = agent4_data.get("test_results", [])

    print(f"\nğŸ§® Agent 4 (Calculation Validation) Metrics:")
    agent4_metrics = set()
    for batch in agent4_tests:
        for result in batch.get("test_execution_batch", {}).get("results", []):
            quality_scores = result.get("quality_scores", {})
            agent4_metrics.update(quality_scores.keys())

    print(f"  Available metrics: {', '.join(sorted(agent4_metrics))}")

    # Extract metrics from Agent 5 data
    agent5_data = sample_data.get("qa_validation_agent_5", {})
    agent5_tests = agent5_data.get("test_results", [])

    print(f"\nâ“ Agent 5 (QA Validation) Metrics:")
    agent5_metrics = set()
    for batch in agent5_tests:
        for result in batch.get("test_execution_batch", {}).get("results", []):
            quality_scores = result.get("quality_scores", {})
            agent5_metrics.update(quality_scores.keys())

    print(f"  Available metrics: {', '.join(sorted(agent5_metrics))}")

    # Check alignment with Quality Control Manager skills
    print(f"\nâš–ï¸  Quality Control Manager Alignment:")
    qc_expected_metrics = {"accuracy", "precision", "reliability", "performance", "completeness", "consistency"}

    agent4_alignment = agent4_metrics & qc_expected_metrics
    agent5_alignment = agent5_metrics & qc_expected_metrics

    print(f"  Agent 4 aligned metrics: {', '.join(sorted(agent4_alignment))}")
    print(f"  Agent 5 aligned metrics: {', '.join(sorted(agent5_alignment))}")
    print(f"  Combined coverage: {len(agent4_alignment | agent5_alignment)}/{len(qc_expected_metrics)} metrics")

    # Test metric extraction
    print(f"\nğŸ” Testing Metric Extraction:")

    try:
        # Test Agent 4 metric extraction
        agent4_summary = await agent._analyze_test_execution(agent4_tests)
        print(f"  Agent 4 analysis:")
        print(f"    Total tests: {agent4_summary.get('total_tests', 0)}")
        print(f"    Success rate: {agent4_summary.get('success_rate', 0):.1%}")
        print(f"    Avg execution time: {agent4_summary.get('average_execution_time', 0):.2f}s")

        # Test Agent 5 metric extraction
        agent5_summary = await agent._analyze_test_execution(agent5_tests)
        print(f"  Agent 5 analysis:")
        print(f"    Total tests: {agent5_summary.get('total_tests', 0)}")
        print(f"    Success rate: {agent5_summary.get('success_rate', 0):.1%}")
        print(f"    Avg execution time: {agent5_summary.get('average_execution_time', 0):.2f}s")

        print(f"  âœ… Metrics extraction successful")

    except Exception as e:
        print(f"  âŒ Metrics extraction failed: {e}")


async def main():
    """Main test function"""

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Quality Control Manager Agent - Reporting & Auditing Test          â•‘
â•‘                                    Agent 6                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Create agent instance
    agent = QualityControlManagerAgent(
        base_url=os.getenv("A2A_SERVICE_URL"),
        data_manager_url=os.getenv("A2A_SERVICE_URL"),
        catalog_manager_url=os.getenv("A2A_SERVICE_URL"),
        enable_monitoring=False  # Disable for testing
    )

    try:
        # Initialize agent
        print("ğŸ”§ Initializing Quality Control Manager Agent...")
        await agent.initialize()
        print("âœ… Agent initialized successfully")

        # Run comprehensive tests
        test_functions = [
            ("Database Integration", test_database_integration),
            ("Comprehensive Reporting", test_comprehensive_reporting),
            ("MCP Reporting Tools", test_mcp_reporting_tools),
            ("Responsible AI Assessment", test_responsible_ai_assessment),
            ("Audit Capabilities", test_audit_capabilities),
            ("Cross-Agent Analysis", test_cross_agent_analysis),
            ("Metrics Alignment", test_metrics_alignment)
        ]

        results = []

        for test_name, test_func in test_functions:
            try:
                print(f"\nğŸ§ª Running {test_name} test...")
                success = await test_func(agent)
                results.append((test_name, "âœ… PASSED" if success != False else "âŒ FAILED"))
                print(f"âœ… {test_name} test completed")
            except Exception as e:
                results.append((test_name, f"âŒ FAILED: {str(e)[:50]}..."))
                print(f"âŒ {test_name} test failed: {e}")

        # Show final test summary
        print(f"\n{'='*80}")
        print(f"Test Results Summary")
        print(f"{'='*80}")

        passed = sum(1 for _, result in results if "PASSED" in result)
        total = len(results)

        for test_name, result in results:
            print(f"  {test_name}: {result}")

        print(f"\nğŸ“Š Overall Results: {passed}/{total} tests passed ({passed/total:.1%})")

        # Show capabilities summary
        print(f"\n{'='*80}")
        print(f"Quality Control Manager Agent Capabilities Verified")
        print(f"{'='*80}")
        print(f"âœ… Database Integration with Agent 4 and 5 data")
        print(f"âœ… Comprehensive reporting (Summary, Detailed, Audit, Compliance)")
        print(f"âœ… MCP tools for report generation and audit summaries")
        print(f"âœ… Responsible AI assessment and bias detection")
        print(f"âœ… Data integrity and process compliance auditing")
        print(f"âœ… Risk assessment and mitigation recommendations")
        print(f"âœ… Cross-agent performance analysis")
        print(f"âœ… Metrics alignment with agent skills")
        print(f"âœ… Stakeholder reporting and executive summaries")

    except Exception as e:
        print(f"âŒ Test suite failed: {e}")
        raise

    finally:
        # Cleanup
        try:
            await agent.shutdown()
            print("\nğŸ›‘ Agent shutdown completed")
        except:
            pass


if __name__ == "__main__":
    asyncio.run(main())
