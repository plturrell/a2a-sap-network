"""
{{ description }}
Generated by Agent Builder Agent on {{ generated_at }}
"""

import asyncio
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from uuid import uuid4
import logging
import time

from app.a2a.sdk import (
    A2AAgentBase, a2a_handler, a2a_skill, a2a_task,
    A2AMessage, MessageRole, create_agent_id
)
from app.a2a.sdk.utils import create_success_response, create_error_response
from app.a2a.core.workflowContext import workflowContextManager
from app.a2a.core.workflowMonitor import workflowMonitor
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# AI/ML specific dependencies
import numpy as np
try:
    import torch
    import transformers
    from sklearn.base import BaseEstimator
    ML_LIBRARIES_AVAILABLE = True
except ImportError:
    ML_LIBRARIES_AVAILABLE = False
    logger.warning("ML libraries not available - some features may be limited")

logger = logging.getLogger(__name__)


class {{ agent_name.replace(' ', '').replace('_', '') }}SDK(A2AAgentBase):
    """
    {{ agent_name }} - SDK Version
    {{ description }}
    """
    
    def __init__(self, base_url: str, **kwargs):
        super().__init__(
            agent_id="{{ agent_id }}",
            name="{{ agent_name }}",
            description="{{ description }}",
            version="1.0.0",
            base_url=base_url
        )
        
        # Configuration
        {% for key, value in configuration.items() %}
        self.{{ key }} = {{ value|tojson if value is string else value|tojson }}
        {% endfor %}
        
        # AI/ML configuration
        self.model_cache_size = kwargs.get('model_cache_size', {{ model_cache_size|default(3) }})
        self.inference_timeout = kwargs.get('inference_timeout', {{ inference_timeout|default(30) }})
        self.device = kwargs.get('device', 'cuda' if torch.cuda.is_available() else 'cpu') if ML_LIBRARIES_AVAILABLE else 'cpu'
        
        # Model management
        self.loaded_models = {}
        self.model_metadata = {}
        
        # Prometheus metrics
        self.tasks_completed = Counter('a2a_agent_tasks_completed_total', 'Total completed tasks', ['agent_id', 'task_type'])
        self.tasks_failed = Counter('a2a_agent_tasks_failed_total', 'Total failed tasks', ['agent_id', 'task_type'])
        self.processing_time = Histogram('a2a_agent_processing_time_seconds', 'Task processing time', ['agent_id', 'task_type'])
        self.queue_depth = Gauge('a2a_agent_queue_depth', 'Current queue depth', ['agent_id'])
        self.skills_count = Gauge('a2a_agent_skills_count', 'Number of skills available', ['agent_id'])
        self.models_loaded = Gauge('a2a_agent_models_loaded', 'Number of models loaded', ['agent_id'])
        self.inference_requests = Counter('a2a_agent_inference_requests_total', 'Total inference requests', ['agent_id', 'model'])
        
        # Set initial metrics
        self.queue_depth.labels(agent_id=self.agent_id).set(0)
        self.skills_count.labels(agent_id=self.agent_id).set({{ skills|length }})
        self.models_loaded.labels(agent_id=self.agent_id).set(0)
        
        # Start metrics server
        self._start_metrics_server()
        
        self.processing_stats = {
            "total_processed": 0,
            "inferences_run": 0,
            "models_trained": 0,
            "models_evaluated": 0,
            {% for skill in skills %}
            "{{ skill }}_count": 0,
            {% endfor %}
        }
        
        logger.info(f"Initialized {self.name} with SDK v1.0.0 on device: {self.device}")
    
    def _start_metrics_server(self):
        """Start Prometheus metrics server"""
        try:
            port = int(os.environ.get('PROMETHEUS_PORT', '8000'))
            start_http_server(port)
            logger.info(f"Started Prometheus metrics server on port {port}")
        except Exception as e:
            logger.warning(f"Failed to start metrics server: {e}")
    
    async def initialize(self) -> None:
        """Initialize agent resources"""
        logger.info("Initializing {{ agent_name }}...")
        
        # Initialize storage
        storage_path = os.getenv("AGENT_STORAGE_PATH", "/tmp/{{ agent_id }}_state")
        os.makedirs(storage_path, exist_ok=True)
        self.storage_path = storage_path
        
        # Initialize model cache directory
        model_cache_path = os.path.join(storage_path, "model_cache")
        os.makedirs(model_cache_path, exist_ok=True)
        self.model_cache_path = model_cache_path
        
        # Custom initialization logic
        await self._custom_initialization()
        
        logger.info("{{ agent_name }} initialization complete")
    
    async def _custom_initialization(self):
        """Custom initialization logic for AI/ML"""
        # Set up ML environment
        if ML_LIBRARIES_AVAILABLE:
            # Configure PyTorch settings
            if torch.cuda.is_available():
                logger.info(f"CUDA available with {torch.cuda.device_count()} devices")
                torch.backends.cudnn.benchmark = True
            
            # Initialize model registry
            self.supported_model_types = ['transformer', 'sklearn', 'custom']
            self.default_models = {}

{% for handler in handlers %}
    @a2a_handler("{{ handler }}")
    async def handle_{{ handler }}(self, message: A2AMessage) -> Dict[str, Any]:
        """Handler for {{ handler }} requests"""
        start_time = time.time()
        
        try:
            # Extract request data
            request_data = self._extract_request_data(message)
            
            # Process request - implement your logic here
            result = await self._process_{{ handler }}(request_data, message.conversation_id)
            
            # Record success metrics
            self.tasks_completed.labels(agent_id=self.agent_id, task_type='{{ handler }}').inc()
            self.processing_time.labels(agent_id=self.agent_id, task_type='{{ handler }}').observe(time.time() - start_time)
            
            return create_success_response(result)
            
        except Exception as e:
            # Record failure metrics
            self.tasks_failed.labels(agent_id=self.agent_id, task_type='{{ handler }}').inc()
            logger.error(f"{{ handler }} failed: {e}")
            return create_error_response(f"{{ handler }} failed: {str(e)}")
    
    async def _process_{{ handler }}(self, request_data: Dict[str, Any], context_id: str) -> Dict[str, Any]:
        """Process {{ handler }} request - implement your logic here"""
        {% if handler == 'ml_prediction' %}
        # ML prediction specific implementation
        model_name = request_data.get('model_name')
        input_data = request_data.get('input_data')
        prediction_config = request_data.get('config', {})
        
        if not model_name or input_data is None:
            raise ValueError("model_name and input_data are required for ML prediction")
        
        # Load model if not already loaded
        model = await self._load_model(model_name)
        
        # Run inference
        predictions = await self._run_inference(model, input_data, prediction_config)
        
        # Update metrics
        self.inference_requests.labels(agent_id=self.agent_id, model=model_name).inc()
        self.processing_stats["inferences_run"] += 1
        
        return {
            "message": "{{ handler }} completed successfully",
            "context_id": context_id,
            "model_name": model_name,
            "predictions": predictions,
            "processing_time": time.time() - start_time,
            "input_shape": np.array(input_data).shape if isinstance(input_data, (list, np.ndarray)) else "unknown"
        }
        
        {% elif handler == 'model_training' %}
        # Model training specific implementation
        training_config = request_data.get('training_config', {})
        training_data = request_data.get('training_data')
        model_type = request_data.get('model_type', 'sklearn')
        
        if not training_data:
            raise ValueError("training_data is required for model training")
        
        # Initialize model
        model = await self._initialize_model(model_type, training_config)
        
        # Train model
        training_results = await self._train_model(model, training_data, training_config)
        
        # Save trained model
        model_path = await self._save_model(model, f"trained_model_{context_id}")
        
        self.processing_stats["models_trained"] += 1
        
        return {
            "message": "{{ handler }} completed successfully",
            "context_id": context_id,
            "model_type": model_type,
            "training_results": training_results,
            "model_path": model_path,
            "training_time": time.time() - start_time
        }
        
        {% else %}
        # Generic handler implementation
        return {"message": "{{ handler }} processed successfully", "context_id": context_id}
        {% endif %}

{% endfor %}

{% for skill in skills %}
    @a2a_skill("{{ skill }}")
    async def {{ skill }}_skill(self, *args, **kwargs) -> Dict[str, Any]:
        """{{ skill }} skill implementation"""
        
        {% if skill == 'model_inference' %}
        # Model inference implementation
        model = kwargs.get('model')
        input_data = kwargs.get('input_data')
        inference_config = kwargs.get('config', {})
        
        if model is None or input_data is None:
            raise ValueError("Model and input_data are required for inference")
        
        try:
            # Run inference based on model type
            if hasattr(model, 'predict'):
                # Scikit-learn style model
                predictions = model.predict(input_data)
            elif hasattr(model, 'forward') or callable(model):
                # PyTorch model
                if ML_LIBRARIES_AVAILABLE:
                    with torch.no_grad():
                        if isinstance(input_data, np.ndarray):
                            input_tensor = torch.tensor(input_data, dtype=torch.float32)
                        else:
                            input_tensor = input_data
                        predictions = model(input_tensor)
                        predictions = predictions.cpu().numpy() if hasattr(predictions, 'cpu') else predictions
                else:
                    raise ValueError("PyTorch not available for model inference")
            else:
                raise ValueError(f"Unknown model type: {type(model)}")
            
            self.processing_stats["{{ skill }}_count"] += 1
            
            return {
                "predictions": predictions.tolist() if hasattr(predictions, 'tolist') else predictions,
                "inference_time": time.time(),
                "input_shape": np.array(input_data).shape if isinstance(input_data, (list, np.ndarray)) else "unknown"
            }
            
        except Exception as e:
            logger.error(f"Model inference failed: {e}")
            raise
        
        {% elif skill == 'data_preprocessing' %}
        # Data preprocessing implementation
        data = kwargs.get('data')
        preprocessing_config = kwargs.get('config', {})
        
        if data is None:
            raise ValueError("Data is required for preprocessing")
        
        processed_data = data
        
        # Apply preprocessing steps
        preprocessing_steps = preprocessing_config.get('steps', ['normalize'])
        
        for step in preprocessing_steps:
            if step == 'normalize':
                processed_data = self._normalize_data(processed_data)
            elif step == 'standardize':
                processed_data = self._standardize_data(processed_data)
            elif step == 'tokenize':
                processed_data = self._tokenize_data(processed_data)
            elif step == 'encode':
                processed_data = self._encode_data(processed_data)
        
        self.processing_stats["{{ skill }}_count"] += 1
        
        return {
            "processed_data": processed_data,
            "preprocessing_steps": preprocessing_steps,
            "output_shape": np.array(processed_data).shape if isinstance(processed_data, (list, np.ndarray)) else "unknown"
        }
        
        {% elif skill == 'result_postprocessing' %}
        # Result postprocessing implementation
        predictions = kwargs.get('predictions')
        postprocessing_config = kwargs.get('config', {})
        
        if predictions is None:
            raise ValueError("Predictions are required for postprocessing")
        
        processed_results = predictions
        
        # Apply postprocessing steps
        postprocessing_steps = postprocessing_config.get('steps', ['format'])
        
        for step in postprocessing_steps:
            if step == 'format':
                processed_results = self._format_predictions(processed_results)
            elif step == 'threshold':
                threshold = postprocessing_config.get('threshold', 0.5)
                processed_results = self._apply_threshold(processed_results, threshold)
            elif step == 'top_k':
                k = postprocessing_config.get('k', 5)
                processed_results = self._get_top_k(processed_results, k)
        
        self.processing_stats["{{ skill }}_count"] += 1
        
        return {
            "results": processed_results,
            "postprocessing_steps": postprocessing_steps,
            "timestamp": datetime.now().isoformat()
        }
        
        {% else %}
        # Generic skill implementation
        self.processing_stats["{{ skill }}_count"] += 1
        return {"result": "{{ skill }} completed", "timestamp": datetime.now().isoformat()}
        {% endif %}

{% endfor %}

{% for task in tasks %}
    @a2a_task(
        task_type="{{ task }}",
        description="{{ task }} task implementation",
        timeout=600,  # Longer timeout for ML tasks
        retry_attempts=2
    )
    async def {{ task }}_task(self, *args, **kwargs) -> Dict[str, Any]:
        """{{ task }} task implementation"""
        
        try:
            {% if task == 'run_inference' %}
            # Run inference task implementation
            model_name = kwargs.get('model_name')
            input_data = kwargs.get('input_data')
            inference_config = kwargs.get('config', {})
            
            if not model_name or input_data is None:
                raise ValueError("model_name and input_data are required")
            
            # Load model
            model = await self._load_model(model_name)
            
            # Preprocess data
            preprocessed_data = await self.execute_skill("data_preprocessing", 
                                                       data=input_data, 
                                                       config=inference_config.get('preprocessing', {}))
            
            # Run inference
            inference_result = await self.execute_skill("model_inference", 
                                                      model=model, 
                                                      input_data=preprocessed_data['processed_data'])
            
            # Postprocess results
            postprocessed_result = await self.execute_skill("result_postprocessing",
                                                          predictions=inference_result['predictions'],
                                                          config=inference_config.get('postprocessing', {}))
            
            result = {
                "model_name": model_name,
                "predictions": postprocessed_result['results'],
                "inference_metadata": {
                    "input_shape": preprocessed_data.get('output_shape'),
                    "preprocessing_steps": preprocessed_data.get('preprocessing_steps'),
                    "postprocessing_steps": postprocessed_result.get('postprocessing_steps')
                }
            }
            
            {% elif task == 'train_model' %}
            # Train model task implementation
            training_data = kwargs.get('training_data')
            model_config = kwargs.get('model_config', {})
            training_config = kwargs.get('training_config', {})
            
            if not training_data:
                raise ValueError("training_data is required")
            
            # Initialize model
            model = await self._initialize_model(model_config.get('type', 'sklearn'), model_config)
            
            # Train model
            training_results = await self._train_model(model, training_data, training_config)
            
            # Save model
            model_name = f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            model_path = await self._save_model(model, model_name)
            
            # Update loaded models
            self.loaded_models[model_name] = model
            self.models_loaded.labels(agent_id=self.agent_id).set(len(self.loaded_models))
            
            result = {
                "model_name": model_name,
                "model_path": model_path,
                "training_results": training_results,
                "model_config": model_config
            }
            
            {% elif task == 'evaluate_model' %}
            # Evaluate model task implementation
            model_name = kwargs.get('model_name')
            test_data = kwargs.get('test_data')
            evaluation_config = kwargs.get('evaluation_config', {})
            
            if not model_name or not test_data:
                raise ValueError("model_name and test_data are required")
            
            # Load model
            model = await self._load_model(model_name)
            
            # Run evaluation
            evaluation_results = await self._evaluate_model(model, test_data, evaluation_config)
            
            self.processing_stats["models_evaluated"] += 1
            
            result = {
                "model_name": model_name,
                "evaluation_results": evaluation_results,
                "test_data_size": len(test_data) if hasattr(test_data, '__len__') else "unknown"
            }
            
            {% else %}
            # Generic task implementation
            result = await self._execute_{{ task }}(*args, **kwargs)
            {% endif %}
            
            self.processing_stats["total_processed"] += 1
            
            return {
                "task_successful": True,
                "task": "{{ task }}",
                "result": result
            }
            
        except Exception as e:
            logger.error(f"{{ task }} task failed: {e}")
            return {
                "task_successful": False,
                "task": "{{ task }}",
                "error": str(e)
            }
    
    async def _execute_{{ task }}(self, *args, **kwargs) -> Dict[str, Any]:
        """Execute {{ task }} - implement your logic here"""
        return {"message": "{{ task }} executed successfully"}

{% endfor %}

    # AI/ML helper methods
    async def _load_model(self, model_name: str):
        """Load model from cache or storage"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        # Check if we need to evict models due to cache size
        if len(self.loaded_models) >= self.model_cache_size:
            # Simple LRU eviction - remove oldest model
            oldest_model = next(iter(self.loaded_models))
            del self.loaded_models[oldest_model]
            logger.info(f"Evicted model {oldest_model} from cache")
        
        # Load model from storage
        model_path = os.path.join(self.model_cache_path, f"{model_name}.pkl")
        if os.path.exists(model_path):
            import pickle
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            self.loaded_models[model_name] = model
            self.models_loaded.labels(agent_id=self.agent_id).set(len(self.loaded_models))
            return model
        else:
            raise ValueError(f"Model {model_name} not found")
    
    async def _save_model(self, model, model_name: str) -> str:
        """Save model to storage"""
        model_path = os.path.join(self.model_cache_path, f"{model_name}.pkl")
        
        import pickle
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        # Update metadata
        self.model_metadata[model_name] = {
            "saved_at": datetime.now().isoformat(),
            "model_type": type(model).__name__,
            "file_path": model_path
        }
        
        return model_path
    
    def _normalize_data(self, data):
        """Normalize data to [0, 1] range"""
        if isinstance(data, (list, np.ndarray)):
            data_array = np.array(data)
            min_val = np.min(data_array)
            max_val = np.max(data_array)
            if max_val - min_val != 0:
                return ((data_array - min_val) / (max_val - min_val)).tolist()
        return data
    
    def _standardize_data(self, data):
        """Standardize data (z-score normalization)"""
        if isinstance(data, (list, np.ndarray)):
            data_array = np.array(data)
            mean = np.mean(data_array)
            std = np.std(data_array)
            if std != 0:
                return ((data_array - mean) / std).tolist()
        return data
    
    def _format_predictions(self, predictions):
        """Format predictions for output"""
        if isinstance(predictions, np.ndarray):
            return predictions.tolist()
        return predictions
    
    def _apply_threshold(self, predictions, threshold):
        """Apply threshold to predictions"""
        if isinstance(predictions, (list, np.ndarray)):
            return [1 if p > threshold else 0 for p in np.array(predictions).flatten()]
        return predictions
    
    def _get_top_k(self, predictions, k):
        """Get top k predictions"""
        if isinstance(predictions, (list, np.ndarray)):
            predictions_array = np.array(predictions).flatten()
            top_k_indices = np.argsort(predictions_array)[-k:][::-1]
            return [{"index": int(idx), "value": float(predictions_array[idx])} for idx in top_k_indices]
        return predictions
    
    def _extract_request_data(self, message: A2AMessage) -> Dict[str, Any]:
        """Extract request data from message"""
        request_data = {}
        
        for part in message.parts:
            if part.kind == "data" and part.data:
                request_data.update(part.data)
            elif part.kind == "file" and part.file:
                request_data["file"] = part.file
        
        return request_data
    
    async def cleanup(self) -> None:
        """Cleanup agent resources"""
        try:
            # Save model metadata
            metadata_file = os.path.join(self.storage_path, "model_metadata.json")
            with open(metadata_file, 'w') as f:
                json.dump(self.model_metadata, f, indent=2)
            
            # Save processing statistics
            stats_file = os.path.join(self.storage_path, "processing_stats.json")
            with open(stats_file, 'w') as f:
                json.dump(self.processing_stats, f, indent=2)
            
            # Clear GPU memory if available
            if ML_LIBRARIES_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info(f"{{ agent_name }} cleanup completed")
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")